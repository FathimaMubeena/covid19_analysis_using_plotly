# -*- coding: utf-8 -*-
"""covid19_analysis_using_plotly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nj0YXEHrGnEjh7R96j9onTi-M3xUWQq8

# Chapter 3: Introduction to Data Analysis in Python Polars
"""

#Install plotly
!pip install plotly

#Install nbformat library to render Plotly visualizations
!pip install nbformat

#Make sure you import the library in your Python code.
# We use Plotly Express in this recipe, which is a high-level interface for Plotly:
import plotly.express as px

"""## Inspecting a DataFrame

### How to do it...
"""

import polars as pl

from google.colab import drive
drive.mount('/content/Drive', force_remount=True)

data_directory = '/content/Drive/MyDrive/python_polars/covid_19_deaths.csv'

df_covid = pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')

#Display the first five rows:
df_covid.head(5)

#Display the last n rows
df_covid.tail(5)

df_covid.glimpse(max_items_per_column=3)# alternative of head(),tail()

#To Check the estimated size of the DataFrame( b, kb, mb, and gb.)
df_covid.estimated_size('mb')

"""How long the data has been captured ?

1. Rename column name Start Date to Start_Date
2. Convert the column: Start Date from str to Date
3. Convert the Date into format yyyy-mm-dd
4. Check the earliest Date and latest Date in the dataset.
"""

# prompt: 1. Rename column name Start Date to Start_Date
# 2. Convert the column: Start Date from str to Date
# 3. Convert the Date into format yyyy-mm-dd
# 4. Check the earliest Date and latest Date in the dataset.

import polars as pl

# Assuming df_covid is already loaded as in your previous code

# 1. Rename column 'Start Date' to 'Start_Date'
df_covid = df_covid.rename({'Start Date': 'Start_Date'})

# 2 & 3. Convert 'Start_Date' to datetime and format as 'yyyy-mm-dd'
df_covid = df_covid.with_columns(
    pl.col("Start_Date").str.strptime(pl.Datetime, "%m/%d/%Y").dt.strftime("%Y-%m-%d")
)

# 4. Check earliest and latest dates
earliest_date = df_covid['Start_Date'].min()
latest_date = df_covid['Start_Date'].max()

print(f"Earliest Date: {earliest_date}")
print(f"Latest Date: {latest_date}")

import polars.selectors as cs
#To Generate summary statistics of the DataFrame
df_covid.select(cs.numeric()).describe()

#To display the count of null values
df_covid.null_count()

"""### There is more..."""

print(df_covid.head())

with pl.Config() as config:
    config.set_tbl_cols(11)
    print(df_covid.head(2))

pl.Config.set_tbl_cols(11)
print(df_covid.head(2))

"""## Casting data types

### How to do it...
"""

import polars as pl

= pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')
df.head()

"""Date and Year columns are read as string. Letâ€™s work on casting those data types to correct ones."""

#Cast data types of certain columns of the DataFrame
df.with_columns(
        pl.col('Data As Of').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('Start Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.to_date('%m/%d/%Y').alias('End Date 2'),
        pl.col('Year').cast(pl.Int64)
).head()
#pl.col('Data As Of'): This selects the column named 'Data As Of' from the DataFrame.
# .str: This indicates we're working with string operations on the selected column.
# strptime: This is a method used to parse a string into a date format.
#pl.Date: Specifies the target data type as Polars Date.
#'%m/%d/%Y': This is the format string that tells strptime how the date is represented in the string (month/day/year).
#.str.strptime(pl.Date, '%m/%d/%Y'): This part handles the data type conversion.
#cast(pl.Int64): This converts the 'Year' column to a 64-bit integer data type.

#If you want to keep the updated DataFrame, then you can simply assign a new variable
updated_df = (
    df.with_columns(
        pl.col('Data As Of').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('Start Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.to_date('%m/%d/%Y').alias('End Date 2'),
        pl.col('Year').cast(pl.Int64)
    )
)

"""##how to cast data types using a LazyFrame in Polars. A LazyFrame is a way to build up a query plan without immediately executing it, which can improve performance for large datasets."""

#Cast date types with a LazyFrame
lf = pl.scan_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')
#This creates a lazy representation of the data, meaning it doesn't load the entire dataset into memory immediately.
lf.with_columns(
        pl.col('Data As Of').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('Start Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.to_date('%m/%d/%Y').alias('End Date 2'),
        pl.col('Year').cast(pl.Int64)
).collect().head()
#collect() is called to trigger the actual execution of the query plan.
#This loads the data into memory and applies all the specified transformations.

"""## Finding and removing duplicates values

### How to do it
"""

import polars as pl

df = pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')
df.head()

df.shape

df.is_duplicated().sum()

df.is_unique().sum()

df.n_unique()

df.select(pl.all().n_unique())
#pl.all():"select all columns." It's like a wildcard.

df.n_unique(subset=['Start Date', 'End Date'])

(
    df.unique(subset=['Start Date', 'End Date'], keep='first')
    .head()
)

rows_to_keep = df.select(['Year', 'COVID-19 Deaths']).is_unique()
rows_to_keep.sum()

df.filter(rows_to_keep).shape
#filter function to keep only the rows in the df DataFrame where
#the corresponding value in the rows_to_keep Series is True (i.e., the unique rows).

df.filter(rows_to_keep).head()

"""### There is more...

.approx_n_unique(): This is a function applied to each selected column (in this case, all columns because of pl.all()). It calculates the approximate number of unique values in each column. It's called "approximate" because it uses an algorithm that's faster for large datasets but might not be perfectly accurate.
"""

df.select(pl.all().approx_n_unique())

"""## Masking sensitive data

### How to do it...
"""

import polars as pl

df = pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')
df.head()

import random

def get_random_nums(num_list, length):
    random_nums = ''.join(str(n) for n in random.sample(num_list, length))
    return random_nums

fake_ssns = []
nums = [n for n in range(10)]

for i in range(df.height):
    part_1 = get_random_nums(nums, 3)
    part_2 = get_random_nums(nums, 2)
    part_3 = get_random_nums(nums, 4)
    fake_ssn = f'{part_1}-{part_2}-{part_3}'
    fake_ssns.append(fake_ssn)

random.seed(10)
fake_ssns_df = pl.DataFrame({'SSN': fake_ssns})
fake_ssns_df.head()

df = pl.concat([df, fake_ssns_df], how='horizontal')

df.select(
    ('XXX-XX-XX' + pl.col('SSN').str.slice(9, 2)).alias('SSN Masked')
).head()

df.select(
    ('XXX-XX-XX' + pl.col('SSN').str.slice(9, 2)).alias('SSN Masked'),

).head()

df.select(
    pl.col('SSN').hash()
).head()

"""## Visualizing data using Plotly

### How to do it...
"""

pip install plotly

import polars as pl
import plotly.express as px

age_groups = ['0-17 years', '18-29 years', '30-39 years', '40-49 years', '50-64 years', '65-74 years', '75-84 years', '85 years and over', 'All Ages']

df = (
    pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv')
    .filter(
        pl.col('Month').is_not_null(),
        pl.col('Age Group').is_in(age_groups),
    )
)
df.head()

#Make sure you import the library in your Python code.
# We use Plotly Express in this recipe, which is a high-level interface for Plotly:
df = (
    pl.read_csv('/content/Drive/MyDrive/python_polars/covid_19_deaths.csv') # Reload the dataframe to ensure original dtypes
    .filter(
        pl.col('Month').is_not_null(),
        pl.col('Age Group').is_in(age_groups),
    )
    .with_columns(
        pl.col('Data As Of').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('Start Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('End Date').str.strptime(pl.Date, '%m/%d/%Y'),
        pl.col('Year').cast(pl.Int64),
        pl.col('Month').cast(pl.Int64)
    )
)
df.head()

"""#Show COVID deaths in 2023 in the US by age group?"""

covid_deaths_by_age = (
    df
    .filter(
        pl.col('State')=='United States',
        pl.col('Year') == 2023,
        pl.col('Age Group') != 'All Ages',
        pl.col('Sex') == 'All Sexes'
    )
    .group_by('Age Group')
    .agg(pl.col('COVID-19 Deaths').sum())
    .sort(by='COVID-19 Deaths', descending=True)
)

fig = px.bar(
    covid_deaths_by_age,
    x='Age Group',
    y='COVID-19 Deaths',
    title='COVID Deaths 2023 by Age Group - As of 9/27/23'
)

fig.update_layout(xaxis_title=None)
fig.show()

"""#Display the number of COVID deaths in 2023 in the US by the top five states?"""

covid_deaths_by_top_5_states = (
    df
    .filter(
        pl.col('State') != 'United States',
        pl.col('Year') == 2023,
        pl.col('Age Group') == 'All Ages',
        pl.col('Sex') == 'All Sexes'
    )
    .group_by('State')
    .agg(pl.col('COVID-19 Deaths').sum())
    .sort(by='COVID-19 Deaths', descending=True)
    .head()
)

fig = px.bar(
    covid_deaths_by_top_5_states,
    x='State',
    y='COVID-19 Deaths',
    title='COVID Deaths 2023 by Top 5 States - As of 9/27/23',
)

fig.update_layout(xaxis_title=None)
fig.show()

"""#Display COVID deaths in 2023 in the US by sex, with data labels?"""

covid_deaths_by_sex = (
    df
    .filter(
        pl.col('State') == 'United States',
        pl.col('Year') == 2023,
        pl.col('Age Group') == 'All Ages',
        pl.col('Sex') != 'All Sexes'
    )
    .group_by('Sex')
    .agg(pl.col('COVID-19 Deaths').sum())
    .sort(by='COVID-19 Deaths', descending=True)
    .head()
)

fig = px.bar(
    covid_deaths_by_sex,
    x='Sex',
    y='COVID-19 Deaths',
    title='COVID Deaths 2023 by Sex - As of 9/27/23',
    text_auto='.2s'
)

fig.update_layout(xaxis_title=None)
fig.update_traces(width = 0.3, textfont_size=12, textangle=0, textposition='inside')
fig.show()

"""#Build a scatterplot to analyze multiple variables. ?"""

!pip install us
import us

!pip install us
!pip install pandas
import us
import pandas as pd
!pip install us
!pip install pandas
import us
import pandas as pd

# Define us_state_division_dict if us_state_mappings module is unavailable
us_state_division_dict = {
    'Alabama': 'East South Central',
    'Alaska': 'Pacific',
    'Arizona': 'Mountain',
    'Arkansas': 'West South Central',
    'California': 'Pacific',
    'Colorado': 'Mountain',
    'Connecticut': 'New England',
    'Delaware': 'South Atlantic',
    'Florida': 'South Atlantic',
    'Georgia': 'South Atlantic',
    'Hawaii': 'Pacific',
    'Idaho': 'Mountain',
    'Illinois': 'East North Central',
    'Indiana': 'East North Central',
    'Iowa': 'West North Central',
    'Kansas': 'West North Central',
    'Kentucky': 'East South Central',
    'Louisiana': 'West South Central',
    'Maine': 'New England',
    'Maryland': 'South Atlantic',
    'Massachusetts': 'New England',
    'Michigan': 'East North Central',
    'Minnesota': 'West North Central',
    'Mississippi': 'East South Central',
    'Missouri': 'West North Central',
    'Montana': 'Mountain',
    'Nebraska': 'West North Central',
    'Nevada': 'Mountain',
    'New Hampshire': 'New England',
    'New Jersey': 'Mid-Atlantic',
    'New Mexico': 'Mountain',
    'New York': 'Mid-Atlantic',
    'North Carolina': 'South Atlantic',
    'North Dakota': 'West North Central'
}

from us_state_mappings import us_state_division_dict

covid_deaths_vs_flu_deaths = (
    df
    .with_columns(
        pl.col('State').replace_strict(us_state_division_dict, default='Others').alias('Division')
    )
    .filter(
        pl.col('State') != 'United States',
        pl.col('Age Group') != 'All Ages',
        pl.col('Sex') != 'All Sexes',
        pl.col('Year') == 2023
    )
    .group_by('State', 'Division')
    .agg(
        pl.col('COVID-19 Deaths').sum(),
        pl.col('Influenza Deaths').sum(),
        pl.col('Pneumonia Deaths').sum()
    )
)

fig = px.scatter(
    covid_deaths_vs_flu_deaths,
    x='COVID-19 Deaths',
    y='Influenza Deaths',
    color='Division',
    size='Pneumonia Deaths',
    hover_name='State',
    title='COVID-19, Influenza, and Pneumonia Deaths 2023 by US States and Divisions'
)

fig.show()

monthly_treand_by_year = (
    df
    .filter(
        pl.col('State') == 'United States',
        pl.col('Age Group') == 'All Ages',
        pl.col('Sex') == 'All Sexes'
    )
    .group_by('Year', 'Month')
    .agg(
        pl.col('COVID-19 Deaths').sum(),
    )
    .sort(by='Month')
)

fig = px.line(
    monthly_treand_by_year,
    x='Month',
    y='COVID-19 Deaths',
    color='Year',
    title='COVID-19 Deaths Monthly Trend - United States',
    line_shape='spline'
)

fig.update_xaxes(dtick = 1)
fig.update_layout(legend_traceorder='reversed')
fig.show()

"""## Detecting and handling outliers

### How to do it...
"""

import polars as pl
import plotly
df = pl.from_pandas(plotly.data.iris())
df.head()

import plotly.express as px

fig = px.box(df, y='sepal_width', width=500)
fig.show()

# Calculate the IQR
q1 = pl.col('sepal_width').quantile(0.25)
q3 = pl.col('sepal_width').quantile(0.75)
iqr = q3 - q1
threshold = 1.5
lower_limit = q1 - iqr * threshold
upper_limit = q3 + iqr * threshold

df.filter(
    (pl.col('sepal_width') < lower_limit) | (pl.col('sepal_width') > upper_limit)
).head()

# remove the outliers
is_outlier_iqr = (pl.col('sepal_width') < lower_limit) | (pl.col('sepal_width') > upper_limit)
df_iqr_outlier_removed = (
    df
    .filter(is_outlier_iqr.not_())
)
df_iqr_outlier_removed.filter(is_outlier_iqr)

"""#Replace the outliers with the median and check there are no outliers after replacing them"""

df_iqr_outlier_replaced = (
    df
    .with_columns(
        pl.when(is_outlier_iqr)
        .then(pl.col('sepal_width').median())
        .otherwise(pl.col('sepal_width'))
        .alias('sepal_width')
    )
)
df_iqr_outlier_replaced.filter(is_outlier_iqr)

"""Confirmed outliers are no longer in the dataset.

#Detect outliers with the z-score. Add a new column that calculates the z-score for every data point for the sepal_length column
"""

df_zscore = (
    df.with_columns(
       sepal_width_zscore=(pl.col('sepal_width') - pl.col('sepal_width').mean()) / pl.col('sepal_width').std()
    )
)
df_zscore.head()

"""#Remove or replace outliers with the z-score."""

#Remove the outlier
is_outlier_z_score = (pl.col('sepal_width_zscore') > 3) | (pl.col('sepal_width_zscore') < -3)
df_zscore_outliers_removed = df_zscore.filter(is_outlier_z_score.not_())

import polars as pl
import plotly

df = pl.from_pandas(plotly.data.iris())
df.head()

# Calculate the z-score and create df_zscore
df_zscore = (
    df.with_columns(
       sepal_width_zscore=(pl.col('sepal_width') - pl.col('sepal_width').mean()) / pl.col('sepal_width').std()
    )
)

# Remove the outlier using df_zscore
is_outlier_z_score = (pl.col('sepal_width_zscore') > 3) | (pl.col('sepal_width_zscore') < -3)
df_zscore_outliers_removed = df_zscore.filter(is_outlier_z_score.not_())

#Check how many points were outliers
df_zscore.filter(is_outlier_z_score)
# show the outlier based on Z- Score

# check the outliers were removed
df_zscore_outliers_removed.filter(is_outlier_z_score)

# Replace the outliers with the. mean
df_zscore_outliers_replaced = (
    df_zscore
    .with_columns(
        pl.when(is_outlier_z_score)
        .then(pl.col('sepal_width').mean())
        .otherwise(pl.col('sepal_width'))
        .alias('sepal_width')
    )
)

df_zscore_outliers_replaced.filter(is_outlier_z_score)

# @title
is_outlier_iqr = (pl.col('sepal_width') < lower_limit) | (pl.col('sepal_width') > upper_limit)